{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thinking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 机器学习中的监督学习、非监督学习、强化学习有何区别\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 监督学习是机器学习的一种，分为两大问题：“回归”和“分类”。\n",
    "#### 在回归问题中，我们会预测一个连续值，比如说预测二手车的交易价格，而分类问题就是预测分类，它与回归问题的区别就在于结果是一个类别，预测结果不是对就是错，而回归问题是对真实值的一种逼近预测，你的预测值与真实值差距越小则越好，不会存在对错的概念，比如预测房价为999元，真实价格为1000元，我们认为这是一个比较好的回归分析。\n",
    "#### 监督学习其实就是根据已有的数据集，知道输入和输出结果之间的关系，根据这种关系训练得到一个最优的模型。监督学习中训练数据是有标签的。\n",
    "#### 监督学习的目的是通过学习许多有标签的样本，然后对新的数据做出预测。\n",
    "\n",
    "#### 无监督学习中，我们基本不知道结果会是什么样，但可以通过聚类的方式从数据中提取一个特殊的结构。在无监督学习中给定的数据集是和监督学习中给定的数据集不一样。无监督学习的训练数据没有相关的标签。\n",
    "#### 无监督学习算法的目标是以某种方式组织数据，然后找出数据中存在的内在结构。这包括将数据进行聚类，或者找到更简单的方式处理复杂数据，使复杂数据看起来更简单。\n",
    "\n",
    "#### 强化学习，又称再励学习或者评价学习，也是机器学习的技术之一。\n",
    "\n",
    "#### 所谓强化学习就是智能系统从环境到行为映射的学习，以使奖励信号（强化信号）函数值最大，由于外部给出的信息很少，强化学习系统必须依靠自身的经历进行自我学习。通过这种学习获取知识，改进行动方案以适应环境。\n",
    "#### 简单来说，强化学习的训练数据没有标签，而是通过环境给出的奖惩来学习. 与监督学习的区别，没有监督学习已经准备好的训练数据输出值，强化学习只有奖励值，但是这个奖励值和监督学习的输出值不一样，它不是事先给出的，而是延后给出的（比如走路摔倒）\n",
    "#### 与非监督学习的区别，在非监督学习中既没有输出值也没有奖励值的，只有数据特征，而强化学习有奖励值（为负是为惩罚），此外非舰队学习与监督学习一样，数据之间也都是独立的，没有强化学习这样的前后依赖关系"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 什么是策略网络，价值网络，有何区别\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 策略网络\n",
    "#### 在一个游戏当中，任何一个动作都会导致状态的变化，我们通常记录为行动a导致了输出的状体s。策略网络就是对于给定的输入，通过学习给出一个确定输出的网络。：（a1,s1）,(a2,s2)，意味着a1动作导致s1状态的输出，a2动作导致s2状态的输出。\n",
    "### 价值网络\n",
    "#### 通过计算目前状态s的累计分数的期望，价值网络给游戏中的状态s赋予一个分值，而奖励更多的状态会在价值网络中被赋予一个更大的分值。\n",
    "#### 所以总结下来，策略网络的输出，是一个落子的概率分布 比如，棋盘中现在轮到白棋走，策略网络会给出下一手的可能性。123456789代表前九位的选点。价值网络的输出，一个可能获胜的数值，即“价值”，这个价值训练是一种回归(regression)，即调整网络的权重来逼近每一种棋局真实的输赢预测\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 请简述MCTS（蒙特卡洛树搜索）的原理，4个步骤Select, Expansion，Simluation，Backpropagation是如何操作的\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 这是一种人工智能问题中做出最优决策的方法，一般是在组合博弈中的行动（move）规划形式。它结合了随机模拟的一般性和树搜索的准确性。\n",
    "#### 1选择 Selection：从根节点 R 开始，递归选择最有价值的子节点直到找到“存在未扩展的子节点”L，即这个局面存在未走过的后续着法的节点，比如3/3节点。\n",
    "#### 2扩展 Expansion：如果 L 不是一个终止节点（也就是，不会导致博弈游戏终止）那么就创建一个或者更多的字子节点，选择其中一个 C。\n",
    "#### 3模拟 Simulation：从 C 开始运行一个模拟的输出，用快速走子策略走到底，得到一个胜负结果。\n",
    "#### 4反向传播 Backpropagation：回传Backup，把模拟的结果加到它的所有父节点上，假设模拟结果是0/1，就把0/1加到所有父节点上。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 假设你是抖音的技术负责人，强化学习在信息流推荐中会有怎样的作用，如果要进行使用强化学习，都有哪些要素需要考虑\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 强化学习可以帮助进行推荐，比如根据用户的反馈作为奖惩，来对推荐系统作强化学习训练，使得推送更为精准\n",
    "#### 需要考虑的要素在于如何给用户的行为定性并且赋予合适的奖惩数值。比如用户在推送的视频上消耗多少时间算是感兴趣，奖励数值给多少。如果点了赞\n",
    "#### 奖励分数又应该是多少。什么样的行为代表用户不感兴趣甚至是厌烦，相应的惩罚分数又是多少。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 在自动驾驶中，如何使用强化学习进行训练，请说明简要的思路\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 首先我们需要把解决的问题转化成一个环境，比如说可以由模拟器将非真实的虚拟图像输入转化到有相似场景的真实图像，让自动驾驶系统进行训练，看是否能完成正常的驾驶功能。\n",
    "#### 自动驾驶的状态空间就是汽车所处的环境，或者详细点说就是车的地理位置，周围的人或建筑物，车轮和地上线的距离等等，尽量将交通状况细化\n",
    "#### 动作空间就是汽车所有前行的方向和速度\n",
    "#### 可行动作：给定一个汽车所处的环境，可以往哪些车道前进或者需要停住，哪些车道不能去或者比较危险\n",
    "#### 状态转化：选择了方向和速度前行后，环境的变化，比如和行人，其他车辆的位置变化\n",
    "#### 奖励函数：前行或者停止后得到的信息反馈，如果没有事故或者到达目的地的预估时间减少，则奖励越高，如果事故发生（虚拟环境），则会有惩罚\n",
    "#### 通过以上的方式来不断训练自动驾驶系统，直到能够安全行驶为止"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Action\n",
    "## 棋盘大小 10 * 10\n",
    "## 采用强化学习（策略价值网络），用AI训练五子棋AI\n",
    "## 请说明都有哪些模块，不同模块的原理\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### game.py，定义了游戏的棋盘、获取棋盘状态，下棋（更新棋盘状态），判断是否有人获胜，绘制棋盘，两个player对弈，自我对弈\n",
    "#### human_play.py，人机对弈，人来输入下棋位置，调用AI进行对战\n",
    "#### mcts_alphaZero.py，实现AlphaGo Zero中的MCTS（蒙特卡洛树搜索），使用了策略网络来指导树搜索并计算叶节点\n",
    "#### mcts_pure.py，实现了随机走子策略的MCTS（蒙特卡洛树搜索）\n",
    "#### policy_value_net_pytorch.py，策略价值网络，用来指导MCTS搜索并计算叶子节点\n",
    "#### train.py，训练AI主程序\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
